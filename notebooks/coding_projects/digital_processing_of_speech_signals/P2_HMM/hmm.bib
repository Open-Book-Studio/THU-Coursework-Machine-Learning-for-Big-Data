@book{LiHang_2019, title={统计学习方法 (第2版)}, ISBN={978-7-302-51727-6}, url={https://book.douban.com/subject/33437381/}, publisher={<a href="https://book.douban.com/press/2562">清华大学出版社</a>    <br>                                                      <span class="pl">出版年:</span> 2019-5}, author={李航}, year={2019}, month=may, language={zh} }
@book{LiHang_2022, title={机器学习方法}, ISBN={978-7-302-59730-8}, url={https://book.douban.com/subject/35884788/}, abstractNote={机器学习是以概率论、统计学、信息论、最优化理论、计算理论等为基础的计算机应用理论学科，也是人工智能、数据挖掘等领域的基础学科。, 《机器学习方法》全面系统地介绍了机器学习的主要方法，共分三篇。第一篇介绍监督学习的主要方法，包括感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、Boosting、EM算法、隐马尔可夫模型、条件随机场等；第二篇介绍无监督学习的主要方法，包括聚类、奇异值分解、主成分分析、潜在语义分析、概率潜在语义分析、马尔可夫链蒙特卡罗法、潜在狄利克雷分配、PageRank 算法等。第三篇介绍深度学习的主要方法，包括前馈神经网络、卷积神经网络、循环神经网络、序列到序列模型、预训练语言模型、生成对抗网络等。, 书中每章介绍一两种机器学习方法，详细叙述各个方法的模型、策略和算法。从具体例子入手，由浅入深，帮助读者直观地理..., (展开全部), 李航，字节跳动科技有限公司人工智能实验室总监，IEEE会士、ACL会士、ACM杰出科学家、CCF杰出会员。研究方向包括信息检索、自然语言处理、统计机器学习及数据挖掘。, 李航于1988年从日本京都大学电气工程系毕业，1998年获得日本东京大学计算机科学博士。他1990年至2001年就职于日本NEC公司中央研究所，任研究员；2001年至2012年就职于微软亚洲研究院，任高级研究员与主任研究员；2012年至2017年就职于华为技术有限公司诺亚方舟实验室，任首席科学家、主任。, 李航一直活跃在相关学术领域，曾出版过四部学术专著，并在国际学术会议和国际学术期刊上发表过120多篇学术论文，包括 SIGIR、WWW、WSDM、ACL、EMNLP、ICML、NIPS、SIGKDD、AAAI、IJCAI，以及 NLE、JMLR、TOIS、IRJ、IPM、TKDE、TWE..., (展开全部)}, publisher={<a href="https://book.douban.com/press/2562">清华大学出版社</a>    <br>                                                      <span class="pl">出版年:</span> 2022-3}, author={李航}, year={2022}, month=mar, language={zh} }
@article{Rabiner_1989, title={A tutorial on hidden Markov models and selected applications in speech recognition}, volume={77}, ISSN={1558-2256}, DOI={10.1109/5.18626}, abstractNote={This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.<>}, note={14240 citations (Crossref) [2024-12-07]}, number={2}, journal={Proceedings of the IEEE}, author={Rabiner, L.R.}, year={1989}, month=feb, pages={257–286}, language={en} }
@inproceedings{Zhang_2024, title={Adaptable Logical Control for Large Language Models}, url={https://openreview.net/forum?id=58X9v92zRd&referrer=%5Bthe%20profile%20of%20Honghua%20Zhang%5D(%2Fprofile%3Fid%3D~Honghua_Zhang1)}, abstractNote={Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation to follow strict constraints at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, a neuro-symbolic framework that enables tractable and adaptable control of LLM generation to follow logical constraints reliably. Ctrl-G combines any production-ready LLM with a Hidden Markov Model (HMM), guiding LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when a TULU2-7B model is coupled with a 2B-parameter HMM, outperforms GPT4 in text editing: on the task of generating text insertions/continuations following logical constraints, our approach achieves over 30% higher satisfaction rate in human evaluation. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts on standard benchmarks by large margins. Additionally, as a proof-of-concept study, we use Ctrl-G to assist LLM reasoning on the GSM benchmark, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.}, author={Zhang, Honghua and Kung, Po-Nien and Yoshida, Masahiro and Broeck, Guy Van den and Peng, Nanyun}, year={2024}, month=nov, language={en} }
@misc{Stochastic_process_2024, rights={Creative Commons Attribution-ShareAlike License}, url={https://en.wikipedia.org/w/index.php?title=Stochastic_process&oldid=1257085679}, abstractNote={In probability theory and related fields, a stochastic () or random process is a mathematical object usually defined as a family of random variables in a probability space, where the index of the family often has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule. Stochastic processes have applications in many disciplines such as biology, chemistry, ecology, neuroscience, physics, image processing, signal processing, control theory, information theory, computer science, and telecommunications. Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.
Applications and the study of phenomena have in turn inspired the proposal of new stochastic processes. Examples of such stochastic processes include the Wiener process or Brownian motion process, used by Louis Bachelier to study price changes on the Paris Bourse, and the Poisson process, used by A. K. Erlang to study the number of phone calls occurring in a certain period of time. These two stochastic processes are considered the most important and central in the theory of stochastic processes, and were invented repeatedly and independently, both before and after Bachelier and Erlang, in different settings and countries.
The term random function is also used to refer to a stochastic or random process, because a stochastic process can also be interpreted as a random element in a function space. The terms stochastic process and random process are used interchangeably, often with no specific mathematical space for the set that indexes the random variables. But often these two terms are used when the random variables are indexed by the integers or an interval of the real line. If the random variables are indexed by the Cartesian plane or some higher-dimensional Euclidean space, then the collection of random variables is usually called a random field instead. The values of a stochastic process are not always numbers and can be vectors or other mathematical objects.
Based on their mathematical properties, stochastic processes can be grouped into various categories, which include random walks, martingales, Markov processes, Lévy processes, Gaussian processes, random fields, renewal processes, and branching processes. The study of stochastic processes uses mathematical knowledge and techniques from probability, calculus, linear algebra, set theory, and topology as well as branches of mathematical analysis such as real analysis, measure theory, Fourier analysis, and functional analysis. The theory of stochastic processes is considered to be an important contribution to mathematics and it continues to be an active topic of research for both theoretical reasons and applications.}, note={Page Version ID: 1257085679}, journal={Wikipedia}, year={2024}, month=nov, language={en} }
