# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb.

# %% auto 0
__all__ = ['fixed_meta_params', 'frozen_rvs', 'study_path', 'sqlite_url', 'study', 'get_rating_matrix', 'get_similarities',
           'compute_weighted_sum_on_matrix', 'get_X_train_weighted', 'ensure_tensor', 'masked_rmse_loss',
           'MatrixFactorization', 'masked_mse_loss', 'jax_masked_mse_loss', 'train_matrix_factorization',
           'draw_metrics_df', 'MatrixFactorizationSetting', 'objective', 'test_normality_small_sample']

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 59
# @joblib_memory.cache # data 哈希本身需要很大的开销，不宜 cache
def get_rating_matrix(data:pd.DataFrame)-> csr_matrix:
    rows = data['user_id'].map(user_to_row)
    cols = data['movie_id'].map(movie_to_col)
    values = data['rating']
    
    return csr_matrix((values, (rows, cols)), shape=(len(user_to_row), len(movie_to_col)))

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 69
from sklearn.metrics.pairwise import cosine_similarity

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 70
@joblib_memory.cache
def get_similarities():
    simularities = cosine_similarity(X_train)
    return simularities

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 82
from copy import deepcopy
from tqdm import tqdm

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 85
def compute_weighted_sum_on_matrix(cosine_sim, X_train_dense_nan):
# 创建一个与 X_train_dense 相同大小的矩阵，用于存储加权平均数
    X_train_weighted = np.zeros_like(X_train_dense_nan)

    # 遍历 X_train_dense 的每一行
    for i in tqdm(range(X_train_dense_nan.shape[0])):
        # 获取第 i 行的权重
        weights = cosine_sim[i, :]
        
        # 复制这个权重到整个矩阵的维度，方便后面掩码操作
        weights = np.repeat(weights, X_train_dense_nan.shape[1]).reshape(X_train_dense_nan.shape[0], X_train_dense_nan.shape[1])
        
        
        # 创建一个掩码，是 nan的就是True
        mask = np.isnan(X_train_dense_nan)
        
        
        # 将权重中的对应位置设置为 np.nan
        weights = np.where(mask, np.nan, weights)
        
        # 计算加权平均数，忽略 np.nan 值
        X_train_weighted[i, :] = np.nansum(X_train_dense_nan * weights, axis=0) / np.nansum(weights, axis=0)

    # X_train_weighted 现在是一个 mxn 的矩阵，其中每一行是忽略 np.nan 的加权平均数
    return X_train_weighted

@joblib_memory.cache
def get_X_train_weighted():
    return compute_weighted_sum_on_matrix(cosine_sim, X_train_dense_nan)

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 104
import torch

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 105
def ensure_tensor(x:np.ndarray|list, device:str="cuda:2"):
    # 确保输入loss函数的数据是张量
    return torch.tensor(x).to(device)
def masked_rmse_loss(reconstructed:torch.Tensor, matrix:torch.Tensor, verbose:bool=False, do_ensure_tensor:bool=True)->torch.Tensor:
    # 首先确保类型是tensor，而且device想同，这样才能在GPU上算Loss
    if do_ensure_tensor:
        reconstructed = ensure_tensor(reconstructed)
        matrix = ensure_tensor(matrix)
    # 获得一个 mask，在老师给的文档中也称为“指示矩阵”
    test_mask = (matrix!= 0)
    n = test_mask.sum()
    if verbose:
        print(f"Number of non-zero elements in the matrix: {n}")
    # 只对mask后的元素做Loss计算
    masked_matrix = matrix[test_mask]
    masked_reconstructed = reconstructed[test_mask]
    return torch.sqrt(torch.mean((masked_reconstructed - masked_matrix)**2))

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 115
import torch
import torch.nn as nn
import torch.optim as optim

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 117
class MatrixFactorization(nn.Module):
    def __init__(self, n_users:int, n_items:int, # 定义分解矩阵的大小
                 k # 隐向量维度
                 ):
        super().__init__()
        self.U = nn.Parameter(torch.randn(n_users, k))
        self.V = nn.Parameter(torch.randn(n_items, k))
    
    def forward(self):
        return torch.matmul(self.U, self.V.t())

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 120
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap # 这三个函数在 jax 中叫做 "transformations", 意思是对函数进行操作的函数（也可以说是泛函、算子），这三个函数分别作用是  求导，即时编译，向量化。
import jax.random as jrandom # 为了和 torch.random 做区分，我们导入叫做 jrandom

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 121
# 再尝试一下jax

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 124
from flax import linen as jnn # 为了和 torch.nn 做区分，我们导入叫做 jnn，和flax官方的写法不同
from flax import nnx # 导入 nnx 库，里面包含了一些常用的网络层
from fastcore.all import store_attr # 导入 fastcore 基础库的 store_attr 函数，用来方便地存储类的属性，这样Python面向对象写起来不那么冗长。 请 pip install fastcore。
import treescope # flax 的 可视化

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 129
# 定义PyTorch的损失函数
def masked_mse_loss(reconstructed:torch.Tensor, matrix:torch.Tensor)->torch.Tensor:
    observed_indices = torch.where(matrix != 0) # A 矩阵，表示哪里是有评分的，只在有评分的地方算loss。
    return 0.5*torch.mean((reconstructed[observed_indices] - matrix[observed_indices])**2)

# 同理，定义 jax 损失函数
def jax_masked_mse_loss(reconstructed:jnp.ndarray, matrix:jnp.ndarray)->jnp.ndarray:
    observed_indices = jnp.where(matrix != 0) # A 矩阵，表示哪里是有评分的，只在有评分的地方算loss。
    return 0.5*jnp.mean((reconstructed[observed_indices] - matrix[observed_indices])**2)

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 136
import lightning as L # PyTorch Lightning库，这里我们只是用它来固定随机数种子
from tqdm import tqdm # 进度条库

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 137
def train_matrix_factorization(X_train_dense:np.array, X_test_dense:np.array, k:int = 50, 
                    lmd:float = 2e-2, lr:float = 5e-3, max_epochs:int = 100000, 
                    required_delta_loss:float = 1e-2,
                    random_state=42, device = 'cuda:4'):
    # 设置 PyTorch 随机种子，防止结果不一致。
    L.seed_everything(random_state) 
    # 输入数据转为 PyTorch 张量
    X_train_torch = torch.from_numpy(X_train_dense).to(device)
    X_test_torch = torch.from_numpy(X_test_dense).to(device)
    # 模型定义
    m, n = X_train_torch.shape
    model = MatrixFactorization(m, n, k).to(device)
    model = torch.compile(model)
    # 优化器定义
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=lmd)
    # 指标
    metrics = []
    # 优化循环
    bar = tqdm(range(max_epochs))
    previous_loss = 0
    for epoch in bar:
        optimizer.zero_grad()
        pred_matrix = model()
        loss = masked_mse_loss(pred_matrix, X_train_torch)
        loss.backward()
        optimizer.step()
        # 计算指标
        with torch.no_grad():
            # RMSE
            train_rmse = masked_rmse_loss(pred_matrix, X_train_torch, do_ensure_tensor=False)
            test_rmse = masked_rmse_loss(pred_matrix, X_test_torch, do_ensure_tensor=False)
            # loss 变化
            loss_item = loss.item()
            delta_loss = abs(loss_item - previous_loss)
            previous_loss = loss_item
            
            metric = dict(
                loss=loss_item,
                train_rmse = train_rmse.item(),
                test_rmse = test_rmse.item(), 
                delta_loss = delta_loss
            )
            # 指标记录
            metrics.append(metric)
            bar.set_postfix(**metric)
            # 中止条件
            if delta_loss<required_delta_loss:
                break
            
    return model, metrics

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 148
import optuna

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 159
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from ..help import plt, pio

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 162
def draw_metrics_df(df:pd.DataFrame, title:str='Metrics'):
    # 使用 plotly 创建图表
    fig = go.Figure()
    # 需要支持第二纵轴
    fig = make_subplots(specs=[[{"secondary_y": True}]], figure=fig)

    # 绘制 loss 曲线
    fig.add_trace(go.Scatter(x=df.index, y=df['loss'], name='Loss', mode='lines'), secondary_y=False)
    fig.add_trace(go.Scatter(x=df.index[1:], y=df['delta_loss'][1:], name='Delta Loss', mode='lines'), secondary_y=False)

    # 绘制 train_rmse 和 test_rmse 曲线
    fig.add_trace(go.Scatter(x=df.index, y=df['train_rmse'], name='Train RMSE', mode='lines'), secondary_y=True)
    fig.add_trace(go.Scatter(x=df.index, y=df['test_rmse'], name='Test RMSE', mode='lines'), secondary_y=True)
    
    # 更新坐标轴
    fig.update_yaxes(title_text="Loss", titlefont_color="red", tickfont_color="red", secondary_y=False)
    fig.update_yaxes(title_text="RMSE", titlefont_color="blue", tickfont_color="blue", secondary_y=True)
    
    # 标题
    fig.update_layout(title=dict(
        text=title
    ),)
    return fig

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 170
from scholarly_infrastructure.rv_args.nucleus import RandomVariable, experiment_setting
from optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution
from typing import Optional, Union
from dataclasses import asdict
import optuna

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 171
@experiment_setting
class MatrixFactorizationSetting:
    # 隐因子数量 k
    k: int = ~RandomVariable( # ~ 表示 服从 这个分布，这是我写的库实现的语法糖
        default=20,
        description="Number of latent factors.",
        # distribution=IntDistribution(low=1, high=128, log=True) # 我们假设 k 和 RMSE满足某种“scaling law”，所以使用 log 分布。
        distribution=CategoricalDistribution([1, 4, 16, 64]) 
        )
    
    # 正则化参数 λ
    lmd: float = ~RandomVariable(
        default=0.001,
        description="Regularization parameter.",
        distribution=FloatDistribution(0.001, 0.1, log=True)
    )
    
    # 学习率
    lr: float = ~RandomVariable(
        default = 5e-3, 
        description="Learning rate.",
        distribution=FloatDistribution(1e-4, 1e-1, log=True) # 虽然本次我们实验不调，但是其固有分布也是要写清楚的，下次实验或许要调。
    )
    
    # 精度要求
    required_delta_loss: float = ~RandomVariable(
        default = 1e-3, 
        description="Required Delta Loss, a.k.a. tolerance",
        distribution=FloatDistribution(1e-8, 1e-1, log=True)
    )      

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 174
fixed_meta_params = MatrixFactorizationSetting(lr=5e-3, # surprise库的值
                                               required_delta_loss=1e-4)
frozen_rvs = {"lr", "required_delta_loss"}

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 176
def objective(trial:optuna.Trial, critical_metric="test_rmse"):
    # experiment_setting 类具有 optuna_suggest 方法，可以自动推荐一个设置出来。
    config:MatrixFactorizationSetting = MatrixFactorizationSetting.optuna_suggest(
        trial, fixed_meta_params, frozen_rvs=frozen_rvs)
    # 调用上面写好的 train_matrix_factorization_jax 函数
    jmodel, jmetrics = train_matrix_factorization_jax(X_train_dense, X_test_dense, 
                                              trial=trial, critical_metric=critical_metric, # 为了进行搜索剪枝，传入这两个参数
                                              **asdict(config))
    # 假如采取最后的
    # best_metric = jmetrics[-1][critical_metric]
    # 假如允许 Early Stop
    best_metric = min(map(lambda m: m[critical_metric], jmetrics))
    return best_metric

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 178
from ..help import runs_path
from optuna.samplers import *
from optuna.pruners import *
import json

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 179
study_path = runs_path / "optuna_studies.db"
sqlite_url = f"sqlite:///{study_path}"

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 180
study = optuna.create_study(
    study_name="matrix factorization hpo 11.27 4.0", # 3.0 使用 1e-3
    storage=sqlite_url, 
    load_if_exists=True, 
    # sampler=QMCSampler(seed=42), # 谷歌建议
    sampler=TPESampler(seed=42), # 谷歌建议
    pruner=HyperbandPruner(), # 通过中间结果来决定是否停止搜索
    direction="minimize")
study.set_user_attr("contributors", ["Ye Canming"])
study.set_user_attr("fixed_meta_parameters", json.dumps(asdict(fixed_meta_params)))

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 207
from ..big_data_analytics.anova import test_normality_group, homogeneity_of_variance
from scipy import stats
from statsmodels.stats.diagnostic import lilliefors
from ..big_data_analytics.anova import auto_anova_for_df, auto_kruskal_for_df

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 208
# 根据上次我们ANOVA作业调研的结果， Shapiro-Wilk 和 Lilliefors 是适合小样本情况下的正态检验方法。
def test_normality_small_sample(df, interesting_col, hue_col='群类别', transform=None):
    if transform is None:
        transform = lambda x: x
    grouped_data = df.groupby(hue_col)
    normality_results = {}
    for name, group in grouped_data:
        normality_result = {}
        data_column = group[interesting_col].to_numpy()
        data_column = transform(data_column)
        
        # Shapiro-Wilk test
        res = stats.shapiro(data_column)
        normality_result['Shapiro-Wilk'] = "Not Normal" if res.pvalue < 0.05 else "Normal"
        
        # Lilliefors test 
        res = lilliefors(data_column)
        normality_result['Lilliefors'] = "Not Normal" if res[1] < 0.05 else "Normal"
        
        normality_results[name] = normality_result
    return pd.DataFrame(normality_results)

# %% ../../notebooks/coding_projects/digital_processing_of_speech_signals/P2_HMM/00hidden_markov_model.ipynb 218
import scikit_posthocs as sp
from scikit_posthocs import posthoc_dunn
